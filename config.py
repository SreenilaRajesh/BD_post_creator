from __future__ import annotations

OLLAMA_HOST = "http://127.0.0.1:11434"
# Reduced-size default model for faster, lighter inference
OLLAMA_MODEL = "phi3:mini"


