{"id": 0, "payload": "Interview Case: Gen AI Social Media Post Creator (PoC)\nObjective\nDevelop a functional proof-of-concept (PoC) web app for creating LinkedIn-style social\nmedia image posts using generative AI. The solution must use Streamlit, Gradio, or\nDash for the UI, and Python for backend logic. All AI components should use open-source\nmodels (e.g., Llama2, Mistral, BLIP2, Stable Diffusion, etc.). The app should\nsupport multimodal RAG (retrieval-augmented generation from PDFs, images, and web\ncontent) and include a web search agent powered by MCP or an agentic AI framework\n(e.g., Crew AI, AutoGen, LangGraph Agents, Langchain etc.).\nNote: Models/Frameworks mentioned are just for reference and any open source\nmodel or framework can be used.\nCore Features\n1. Image Upload & Creation\n• Users can upload their own images or generate images via an AI prompt (using\nopen-source models like Stable Diffusion, with proper code and placeholders if no\naccess).\n• Optionally, users can generate images based on conten"}
{"id": 1, "payload": " proper code and placeholders if no\naccess).\n• Optionally, users can generate images based on content extracted from PDFs or\nweb search results.\n2. Text Boxes with AI Assistance\n• Each text box can be filled by:\no AI-generated text (using a prompt, PDF content, or web search agent).\n3. Multimodal RAG Integration\n• Users can upload a PDF (e.g., resume, brochure) containing tables, charts,images,\nand text.\n• System extracts relevant content from the PDF using RAG (LangChain, LlamaIndex,\nHaystack, etc.).\n• Users can provide a web URL or search query; the system uses a web search\nagent (MCP or agentic AI framework) to retrieve and summarize relevant content.\n• All retrieved content (text, images) can be used for post/caption generation.\n\n4. Slideshow/Post Series\n• Support for multiple slides/images in a post.\n• Each slide/image allows for AI-generated or user-generated captions and\ndescriptions.\n• Allow rearranging slides and previewing the post as a sequence.\n5. Preview & Download\n• Previ"}
{"id": 2, "payload": "ons.\n• Allow rearranging slides and previewing the post as a sequence.\n5. Preview & Download\n• Preview the post as it would appear on LinkedIn.\n• Option to download the final composed post (single image or slideshow as images).\nTechnical Stack\n• UI: Streamlit, Gradio, or Dash (Python-based, rapid prototyping, supports\nmultimodal inputs/outputs)\n• AI Layer:\no LangChain, LlamaIndex, Haystack, DSPy, or LangGraph for orchestration\no LLM: Open-source (Llama2, Mistral, Falcon, GPT4All, etc.)\no Multimodal models: BLIP2, LLaVA, Stable Diffusion, Hugging Face Vision\nTransformers\n• Image Generation: Stable Diffusion or similar open-source models (API call or\nstub/mock if no access)\n• PDF Processing: PyPDF, pdfplumber, or similar, integrated with LangChain for RAG\n• Web Search Agent: MCP (Model Context Protocol) or agentic AI framework (Crew\nAI, AutoGen, LangChain Agents) for web content retrieval and summarization\n• Vector Database: FAISS, Qdrant, Pinecone, or Weaviate (for embeddings and\nretrie"}
{"id": 3, "payload": "and summarization\n• Vector Database: FAISS, Qdrant, Pinecone, or Weaviate (for embeddings and\nretrieval)\n• NLP: Hugging Face Transformers, spaCy for text processing and entity extraction\n\nExpected Deliverables\n1. Working Code\no Streamlit, Gradio, or Dash app (Python)\no All core features implemented (functional with real AI models or proper\nplaceholders)\no Well-structured and commented code\n2. Documentation\no Clear README with setup instructions\no How to insert API keys if available\no Brief note on how you would productionize or scale this system\n3. Demo\no Short demo (screenshots, GIF, or short video) showing main features and\nworkflow\nBonus (Optional but adds value in evaluation)\n• Authentication for users\n• Multi-language support\n• Docker setup\n• Error handling and user-friendly messages\n• Advanced multimodal features (e.g., image captioning, OCR, text-to-image with\ncontext from PDF/web)\n• Integration with Azure AI Studio, Azure Machine Learning, or Azure Databricks\n(mocked if no acce"}
{"id": 4, "payload": ")\n• Integration with Azure AI Studio, Azure Machine Learning, or Azure Databricks\n(mocked if no access)\nEvaluation Criteria\n• Completeness and functionality of the PoC (core features met)\n• Code quality, organization, and documentation\n\n• Clarity and usability of the UI/UX\n• Proper AI integration or clear placeholders\n• Approach to multimodal RAG (PDF, web, images) and AI content generation\n• Any extra innovative or useful feature added not mentioned\nTimeframe\nYou have 4 working days to complete and submit your solution. Good luck! We are looking\nforward to seeing your creativity and technical skills in action."}
{"id": 5, "payload": "AUTODESK GETML TEAM\nASSIGNMENT\nSreenila Rajesh\n\nContext: Domain-specific Question Answering chatbot (customer\nsupport / internal knowledge base)\nR\nProblem: Vanilla LLMs hallucinate, use stale info, and can’t cite\nProblem &\nsources\nGoal\nGoal: Use Retrieval-Augmented Generation (RAG) to ground\nresponses in domain documents so answers are accurate, up-to-\nCreate a RAG conversational\ndate, and traceable.\nchatbot that can answer typical\ncustomer questions from the\ncontent provided\nWhy RAG\nResult: Lower hallucination, improved factual accuracy, Source citation\n\nData Source and Preparation\nSources: HTML pages\nImportant metadata to capture: URL/path\nGoal: produce an up-to-date, searchable corpus\nfor retrieval\nSTEPS\n1.Convert HTML documents to Markdown format\n2.Chunk the documents\nWHY MARKDOWN?\nPreserves structure (headings, lists, etc) but removes\nunnecessary style/layout info\nWHY CHUNK?\nModel token limit and semantic relevance\n\nVector Database and\nEmbeddings\nEmbedding Model: ColBERTv2 – late "}
{"id": 6, "payload": "oken limit and semantic relevance\n\nVector Database and\nEmbeddings\nEmbedding Model: ColBERTv2 – late interaction\nmodel for high-precision retrieval on small\ncorpora.\nVector Store: Qdrant (self-hosted) – lightweight,\nfast, supports HNSW indexing with metadata\nfiltering.\nIndex Type: HNSW (Hierarchical Navigable Small\nWorld) – efficient ANN search, tuned for low-\nlatency queries.\nCorpus Size: 1200+ domain-specific documents,\n11.7 k chunks\n\nRetriever and Generator\nRetriever:\nQdrant (self-hosted) + HNSW index\nColBERTv2 embeddings for fine-grained semantic matching\nRetrieves top-k = 5 most relevant chunks with metadata\nPrompt Builder:\nMerges system prompt, user query, and retrieved context\nAdds instructions to reduce hallucination\nGenerator:\nLLaMA 3–8B (local) for low-latency inference\nOutputs factual, context-grounded answers with citations\n\nFull Pipeline\n\nResponses on Sample queries\n\nResponses on Sample queries\n\nResults\nReduced Hallucinations: Answers grounded in\nretrieved domain-specific c"}
{"id": 7, "payload": "s on Sample queries\n\nResults\nReduced Hallucinations: Answers grounded in\nretrieved domain-specific content.\nImproved Explainability: Citations provided for each\nfactual statement.\nModel Strengths: Performs well on direct, single-fact\nqueries.\nModel Weakness: Struggles with multi-hop reasoning\n(combining multiple facts).\nLatency Concern: Overall high response time in\ncurrent setup (~33 sec avg).\n\nChallenges and Next steps\nIndexer Optimizer: Number of documents and Size of documents is\nsmall, hence smaller embedding models like Sentence Transformers\n(e.g., all-MiniLM-L6-v2, multi-qa-MiniLM) can be tried\nRetriever Optimization: Enable hybrid search, add caching.\nLLM Efficiency: Test smaller/lighter models for reduced latency.\nMulti-hop Reasoning: Integrate query decomposition or graph-based\nretrieval.\nMulti-turn Memory: Implement conversation history for context\ncontinuity.\nScaling Dataset: Stress-test with more than 1K docs to ensure\nindexing/retrieval remains performant."}
